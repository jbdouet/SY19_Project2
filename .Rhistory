#layer_dropout(rate = 0.01) %>%
#layer_dense(units = 128, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
X_train, y_train,
epochs = 100, batch_size = 128,
validation_split = 0.2
)
model <- keras_model_sequential()
model %>%
layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
#layer_dropout(rate = 0.01) %>%
#layer_dense(units = 128, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
X_train, y_train,
epochs = 100, batch_size = 128,
validation_split = 0.2
)
model <- keras_model_sequential()
model %>%
layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
X_train, y_train,
epochs = 100, batch_size = 128,
validation_split = 0.2
)
model %>%
layer_dense(units = 1, activation = 'relu', input_shape = c(4200)) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
model <- keras_model_sequential()
model %>%
layer_dense(units = 1, activation = 'relu', input_shape = c(4200)) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
X_train, y_train,
epochs = 100, batch_size = 128,
validation_split = 0.2
)
model <- keras_model_sequential()
model %>%
layer_dense(units = 1, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
model <- keras_model_sequential()
model %>%
layer_dense(units = 1, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
model %>%
layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(3,3)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
model %>%
layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(3,3),input_shape = c(4200)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
dim(X_train)
X_train2<- array_reshape(unname(X_train), dim=c(dim(X_train)[1], 60,70))
X_test2<- array_reshape(unname(X_test), dim=c(dim(X_test)[1], 60,70))
model <- keras_model_sequential()
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(3,3),input_shape = c(4200)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(3,3),input_shape = c(60, 70, 1)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
X_train, y_train,
epochs = 100, batch_size = 128,
validation_split = 0.2
)
history <- model %>% fit(
X_train2, y_train,
epochs = 100, batch_size = 128,
validation_split = 0.2
)
dim(X_train2)
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
model <- keras_model_sequential()
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(3,3),input_shape = c(60, 70, 1)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32,input_shape = c(60, 70, 1)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(1,1),input_shape = c(60, 70, 1)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
model <- keras_model_sequential()
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(1,1),input_shape = c(60, 70, 1)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = opt,
metrics = c('accuracy')
)
history <- model %>% fit(
X_train2, y_train,
epochs = 100, batch_size = 128,
validation_split = 0.2
)
history <- model %>% fit(
X_train2, y_train,
epochs = 100, batch_size = 3,
validation_split = 0.2
)
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(1,1),input_shape = c(60, 70, 1)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = opt,
metrics = c('accuracy')
)
history <- model %>% fit(
X_train2, y_train,
epochs = 100, batch_size = 3,
validation_split = 0.2
)
model %>% evaluate(X_test, y_test)
X_train <- data_expressions.train[,1:4200]
X_test <- data_expressions.test[,1:4200]
y_train <-  data_expressions.train$y
y_test <-  data_expressions.test$y
X_train<- array_reshape(unname(X_train), dim=c(dim(X_train)[1], dim(X_train)[2]))
X_test<- array_reshape(unname(X_test), dim=c(dim(X_test)[1], dim(X_test)[2]))
X_train <- X_train / 255
X_test <- X_test / 255
dim(X_train2)
X_train2<- array_reshape(unname(X_train), dim=c(dim(X_train)[1], 60,70))
X_test2<- array_reshape(unname(X_test), dim=c(dim(X_test)[1], 60,70))
y_train <- to_categorical(y_train)
y_test <- to_categorical(y_test)
y_train <- y_train[,2:7]
y_test <- y_test[,2:7]
model <- keras_model_sequential()
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(1,1),input_shape = c(60, 70, 1)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = opt,
metrics = c('accuracy')
)
history <- model %>% fit(
X_train2, y_train,
epochs = 100, batch_size = 3,
validation_split = 0.2
)
X_train2<- array_reshape(unname(X_train), dim=c(dim(X_train)[1], 60,70,1))
X_test2<- array_reshape(unname(X_test), dim=c(dim(X_test)[1], 60,70,1))
y_train <- to_categorical(y_train)
y_test <- to_categorical(y_test)
y_train <- y_train[,2:7]
y_test <- y_test[,2:7]
y_train <-  data_expressions.train$y
y_test <-  data_expressions.test$y
X_train<- array_reshape(unname(X_train), dim=c(dim(X_train)[1], dim(X_train)[2]))
X_test<- array_reshape(unname(X_test), dim=c(dim(X_test)[1], dim(X_test)[2]))
X_train <- X_train / 255
X_test <- X_test / 255
dim(X_train2)
X_train2<- array_reshape(unname(X_train), dim=c(dim(X_train)[1], 60,70,1))
X_test2<- array_reshape(unname(X_test), dim=c(dim(X_test)[1], 60,70,1))
y_train <- to_categorical(y_train)
y_test <- to_categorical(y_test)
y_train <- y_train[,2:7]
y_test <- y_test[,2:7]
model <- keras_model_sequential()
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(1,1),input_shape = c(60, 70, 1)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = opt,
metrics = c('accuracy')
)
history <- model %>% fit(
X_train2, y_train,
epochs = 100, batch_size = 3,
validation_split = 0.2
)
X_train <- data_expressions.train[,1:4200]
X_test <- data_expressions.test[,1:4200]
y_train <-  data_expressions.train$y
y_test <-  data_expressions.test$y
X_train1<- array_reshape(unname(X_train), dim=c(dim(X_train)[1], dim(X_train)[2]))
X_test1<- array_reshape(unname(X_test), dim=c(dim(X_test)[1], dim(X_test)[2]))
X_train1 <- X_train / 255
X_test1 <- X_test / 255
img_rows <- 60
img_cols <- 70
X_train <- data_expressions.train[,1:4200]
X_test <- data_expressions.test[,1:4200]
y_train <-  data_expressions.train$y
y_test <-  data_expressions.test$y
X_train1<- array_reshape(unname(X_train), dim=c(dim(X_train)[1], dim(X_train)[2]))
X_test1<- array_reshape(unname(X_test), dim=c(dim(X_test)[1], dim(X_test)[2]))
X_train1 <- X_train / 255
X_test1 <- X_test / 255
X_train2 <- array_reshape(X_train, c(nrow(X_train), img_rows, img_cols, 1))
X_train2 <- array_reshape(unname(X_train), c(nrow(X_train), img_rows, img_cols, 1))
X_train2 <- array_reshape(unname(X_train), c(nrow(X_train), img_rows, img_cols, 1))
X_test2 <- array_reshape(unname(X_test), c(nrow(x_test), img_rows, img_cols, 1))
X_test2 <- array_reshape(unname(X_test), c(nrow(X_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
y_train <- to_categorical(y_train)
y_test <- to_categorical(y_test)
y_train <- to_categorical(y_train)
y_test <- to_categorical(y_test)
y_train <- y_train[,2:7]
y_train <- to_categorical(y_train)
y_test <- to_categorical(y_test)
y_train <- y_train[,2:7]
y_test <- y_test[,2:7]
y_train <-  data_expressions.train$y
y_test <-  data_expressions.test$y
y_train_cat <- to_categorical(y_train)
y_test_cat <- to_categorical(y_test)
y_train_cat2 <- y_train[,2:7]
y_test_cat2 <- y_test[,2:7]
y_train_cat2 <- y_train_cat[,2:7]
y_test_cat2 <- y_test_cat[,2:7]
model <- keras_model_sequential()
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filter = 32, kernel_size = c(1,1),input_shape = c(60, 70, 1)) %>%
layer_activation("relu") %>%
# Use max pooling
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(0.25) %>%
#layer_dropout(rate = 0.2) %>%
#layer_dense(units = 10, activation = 'relu') %>%
#layer_dropout(rate = 0.3) %>%
layer_dense(units = 6, activation = 'softmax')
summary(model)
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = opt,
metrics = c('accuracy')
)
history <- model %>% fit(
X_train2, y_train_cat2,
epochs = 100, batch_size = 3,
validation_split = 0.2
)
history <- model %>% fit(
X_train2, y_train_cat2,
epochs = 100, batch_size = 1,
validation_split = 0.2
)
num_classes=6
model <- keras_model_sequential()
model %>%
#layer_dense(units = 10, activation = 'relu', input_shape = c(4200)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
summary(model)
opt <- optimizer_rmsprop(lr = 0.0001, decay = 1e-6)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = opt,
metrics = c('accuracy')
)
history <- model %>% fit(
X_train2, y_train_cat2,
epochs = 100, batch_size = 1,
validation_split = 0.2
)
data_expressions$y
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = ntrain)) # !!! le ntrain doit correspondre à la taille du dataset que l'on utilisera dans la boucle de cross validation
table(folds_i) # Pas le même nombre d'éléments
CV<-rep(0,10)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
train_xy <- data_expressions[-test_i, ]
test_xy <- data_expressions[test_i, ]
print(k)
model_svmLinear <- caret::train(train_xy[,1:4200],train_xy$y,method='svmLinear',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
predictions_svmLinear<-predict.train(object=model_svmLinear,test_xy[,1:4200])
cf_svmLinear<-caret::confusionMatrix(data= predictions_svmLinear,reference=test_xy$y)
CV[k]<- cf_svmLinear$overall["Accuracy"]
}
library(caret)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
train_xy <- data_expressions[-test_i, ]
test_xy <- data_expressions[test_i, ]
print(k)
model_svmLinear <- caret::train(train_xy[,1:4200],train_xy$y,method='svmLinear',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
predictions_svmLinear<-predict.train(object=model_svmLinear,test_xy[,1:4200])
cf_svmLinear<-caret::confusionMatrix(data= predictions_svmLinear,reference=test_xy$y)
CV[k]<- cf_svmLinear$overall["Accuracy"]
}
CVerror= sum(CV)/length(CV)
CV
CVerror
cf
n_folds <- 1
folds_i <- sample(rep(1:n_folds, length.out = 36)) # !!! le ntrain doit correspondre à la taille du dataset que l'on utilisera dans la boucle de cross validation
table(folds_i) # Pas le même nombre d'éléments
CV<-rep(0,n_folds)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
train_xy <- data_expressions[-test_i, ]
test_xy <- data_expressions[test_i, ]
print(k)
model_svmRadial <- caret::train(train_xy[,1:4200],train_xy$y,method='mlp',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
predictions_svmRadial<-predict.train(object=model_svmRadial,test_xy[,1:4200])
cf_svmRadial<-caret::confusionMatrix(data= predictions_svmRadial,reference=test_xy$y)
CV[k]<- cf_svmLinear$overall["Accuracy"]
}
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
train_xy <- data_expressions[-test_i, ]
test_xy <- data_expressions[test_i, ]
print(k)
model_svmRadial <- caret::train(train_xy[,1:4200],train_xy$y,method='lssvmPoly',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
predictions_svmRadial<-predict.train(object=model_svmRadial,test_xy[,1:4200])
cf_svmRadial<-caret::confusionMatrix(data= predictions_svmRadial,reference=test_xy$y)
CV[k]<- cf_svmLinear$overall["Accuracy"]
}
cf_svmRadial
CVerror= sum(CV)/length(CV)
CV
CVerror
