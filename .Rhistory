################################################################################################
#########################################CHARACTER##############################################
################################################################################################
################################################################################################
library(caret)
library(car)
library("e1071")
library(randomForest)
library(kernlab)
library(stats)
character_data <- read.csv("data/characters_train.txt", sep =" ")
character <- read.csv("data/characters_train.txt", sep =" ")
n=nrow(character)
setwd("~/SY19/SY19_Project2")
character <- read.csv("data/characters_train.txt", sep =" ")
n=nrow(character)
ntrain=ceiling(n*2/3)
ntst=n-ntrain
train<-sample(1:n,ntrain)
character.test<-character[-train,]
character.train<-character[train,]
#LDA
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) # !!! le ntrain doit correspondre à la taille du dataset que l'on utilisera dans la boucle de cross validation
table(folds_i) # Pas le même nombre d'éléments
CV<-rep(0,10)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
train_xy <- character[-test_i, ]
test_i <- which(folds_i == k)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
train_xy <- character[-test_i, ]
test_xy <- character[test_i, ]
print(k)
# model_lda <- train(train_xy[,-1],train_xy$Y,method='lda',trControl= trainControl(
#  method = "cv",
#  number =10,
#  verboseIter = TRUE))
#predictions_lda<-predict.train(object=model_lda,test_xy[,-1])
pc <- prcomp(train_xy[,-1], center = TRUE)
mydata <- data.frame(Species = train_xy[, 1], pc$x)
t<-lda(Species~PC1:PC2, data = mydata)
test.p <- predict(pc, newdata = test_xy[, -1])
pred_svm <- predict(t, newdata = data.frame(test.p), type = "response")
prop.table(table(test_xy$y,pred_svm))
cm= as.matrix(table(test_xy$y,pred_svm))
CV[k]<- sum(diag(cm)) / sum(cm)
}
library(car)
library(cran)
install.packages('cran')
install.packages(cran)
install.packages(CRAN)
install.packages("CRAN")
library(MASS)
t<-lda(Species~PC1:PC2, data = mydata)
test.p <- predict(pc, newdata = test_xy[, -1])
pred_svm <- predict(t, newdata = data.frame(test.p), type = "response")
prop.table(table(test_xy$y,pred_svm))
prop.table(table(test_xy$y,pred_svm$class))
prop.table(table(test_xy$Y,pred_svm$class))
cm= as.matrix(table(test_xy$Y,pred_svm$class))
#LDA
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) # !!! le ntrain doit correspondre à la taille du dataset que l'on utilisera dans la boucle de cross validation
table(folds_i) # Pas le même nombre d'éléments
CV<-rep(0,10)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
train_xy <- character[-test_i, ]
test_xy <- character[test_i, ]
print(k)
# model_lda <- train(train_xy[,-1],train_xy$Y,method='lda',trControl= trainControl(
#  method = "cv",
#  number =10,
#  verboseIter = TRUE))
#predictions_lda<-predict.train(object=model_lda,test_xy[,-1])
pc <- prcomp(train_xy[,-1], center = TRUE)
mydata <- data.frame(Species = train_xy[, 1], pc$x)
t<-lda(Species~PC1:PC2, data = mydata)
test.p <- predict(pc, newdata = test_xy[, -1])
pred_svm <- predict(t, newdata = data.frame(test.p), type = "response")
prop.table(table(test_xy$Y,pred_svm$class))
cm= as.matrix(table(test_xy$Y,pred_svm$class))
CV[k]<- sum(diag(cm)) / sum(cm)
}
CVerror= sum(CV)/length(CV)
CV
CVerror#0.70
# model_lda <- train(train_xy[,-1],train_xy$Y,method='lda',trControl= trainControl(
#  method = "cv",
#  number =10,
#  verboseIter = TRUE))
#predictions_lda<-predict.train(object=model_lda,test_xy[,-1])
pc <- prcomp(train_xy[,-1], center = TRUE)
pc$sdev
#LDA
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) # !!! le ntrain doit correspondre à la taille du dataset que l'on utilisera dans la boucle de cross validation
table(folds_i) # Pas le même nombre d'éléments
CV<-rep(0,10)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
train_xy <- character[-test_i, ]
test_xy <- character[test_i, ]
print(k)
# model_lda <- train(train_xy[,-1],train_xy$Y,method='lda',trControl= trainControl(
#  method = "cv",
#  number =10,
#  verboseIter = TRUE))
#predictions_lda<-predict.train(object=model_lda,test_xy[,-1])
pc <- prcomp(train_xy[,-1], center = TRUE)
mydata <- data.frame(Species = train_xy[, 1], pc$x)
t<-lda(Species~PC1:PC5, data = mydata)
test.p <- predict(pc, newdata = test_xy[, -1])
pred_svm <- predict(t, newdata = data.frame(test.p), type = "response")
prop.table(table(test_xy$Y,pred_svm$class))
cm= as.matrix(table(test_xy$Y,pred_svm$class))
CV[k]<- sum(diag(cm)) / sum(cm)
}
CVerror= sum(CV)/length(CV)
CV
CVerror#0.70
trans<- preProcess(train_xy, method = 'pca')
trans$pcaComp
trans$dim
trans$rotation
help("prcomp")
trans<- preProcess(train_xy, method = 'pca')
t<- train(trans$rotation,train_xy$Y, method = 'lda',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
length(trans$rotation)
length(train_xy$Y)
trans = preProcess(train_xy[,-1],
method=c("BoxCox", "center",
"scale", "pca"))
PC = predict(trans, train_xy[,-1])
PC = predict(trans, train_xy[,-1])
trans$rotation
PC = predict(trans, test_xy[,-1])
PC = predict(trans, test_xy)
test_xy[,-1]
PC = predict(trans, test_xy)
#LDA
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) # !!! le ntrain doit correspondre à la taille du dataset que l'on utilisera dans la boucle de cross validation
table(folds_i) # Pas le même nombre d'éléments
CV<-rep(0,10)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent être les mêmes car c'est le même dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idée finale de la performance sur le test
train_xy <- character[-test_i, ]
test_xy <- character[test_i, ]
print(k)
model_lda <- train(train_xy[,-1],train_xy$Y,method='lda',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
predictions_lda<-predict.train(object=model_lda,test_xy[,-1])
cf<-confusionMatrix(predictions_lda,test_xy$Y)
CV[k]<- cf$overall["Accuracy"]
}
CVerror= sum(CV)/length(CV)
CV
CVerror#0.70
model_lda <- train(train_xy[,-1],train_xy$Y,method=c('pca','qda'),trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
model_lda <- train(train_xy[,-1],train_xy$Y,method='qda',trControl= trainControl(
method = "cv",
number =10,
verboseIter = TRUE))
parole <- read.csv("data/parole_train.txt", sep =" ")
n=nrow(parole)
ntrain=ceiling(n*2/3)
ntst=n-ntrain
train<-sample(1:n,ntrain)
parole.test<-parole[-train,]
parole.train<-parole[train,]
pc <- prcomp(train_xy[,-257], center = TRUE)
#SVM +CV
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) # !!! le ntrain doit correspondre Ã  la taille du dataset que l'on utilisera dans la boucle de cross validation
table(folds_i) # Pas le mÃªme nombre d'Ã©lÃ©ments
CV<-rep(0,10)
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent Ãªtre les mÃªmes car c'est le mÃªme dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idÃ©e finale de la performance sur le test
train_xy <- parole[-test_i, ]
test_xy <- parole[test_i, ]
pc <- prcomp(train_xy[,-257], center = TRUE)
mydata <- data.frame(Species = train_xy[, 257], pc$x)
t<-svm(Species~PC1:PC2, data = mydata)
#test.p <- predict(pc, newdata = test_xy[, -257])
pred_svm <- predict(t, newdata = test_xy, type = "response")
svm_train <- naiveBayes(Y ~ ., data = train_xy)
#Naive Bayes + double CV
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) # !!! le ntrain doit correspondre Ã  la taille du dataset que l'on utilisera dans la boucle de cross validation
table(folds_i) # Pas le mÃªme nombre d'Ã©lÃ©ments
CV<-rep(0,10)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent Ãªtre les mÃªmes car c'est le mÃªme dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idÃ©e finale de la performance sur le test
train_xy <- character[-test_i, ]
test_xy <- character[test_i, ]
print(k)
#model_naive_bayes <- train(train_xy[,-1],train_xy$Y,method='nb',trControl= trainControl(
#  method = "cv",
#  number =10,
#  verboseIter = TRUE))
# predictions_naive_bayes<-predict.train(object=model_naive_bayes,test_xy[,-1])
svm_train <- naiveBayes(Y ~ ., data = train_xy)
pred_svm<-predict(svm_train, newdata = test_xy, type = "response")
prop.table(table(test_xy$Y,pred_svm))
cm= as.matrix(table(test_xy$Y,pred_svm))
CV[k]<- sum(diag(cm)) / sum(cm)
#cf<-confusionMatrix(predictions_naive_bayes,test_xy$Y)
#CV[k]<- cf$overall["Accuracy"]
}
svm_train <- naiveBayes(Y ~ ., data = train_xy)
pred_svm<-predict(svm_train, newdata = test_xy, type = "response")
svm_train
pred_svm<-predict(svm_train, newdata = test_xy, type = "response")
nb_train <- naiveBayes(Y ~ ., data = train_xy)
pred_svm<-predict(nb_train, newdata = test_xy)
prop.table(table(test_xy$Y,pred_svm))
cm= as.matrix(table(test_xy$Y,pred_svm))
CV[k]<- sum(diag(cm)) / sum(cm)
#Naive Bayes + double CV
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) # !!! le ntrain doit correspondre Ã  la taille du dataset que l'on utilisera dans la boucle de cross validation
table(folds_i) # Pas le mÃªme nombre d'Ã©lÃ©ments
CV<-rep(0,10)
for (k in 1:n_folds) {# we loop on the number of folds, to build k models
test_i <- which(folds_i == k)
# les datasets entre le fit et le predict doivent Ãªtre les mÃªmes car c'est le mÃªme dataset que l'on divise en k-fold
# on peut utiliser le data set complet ou seulement le train et avoir une idÃ©e finale de la performance sur le test
train_xy <- character[-test_i, ]
test_xy <- character[test_i, ]
print(k)
#model_naive_bayes <- train(train_xy[,-1],train_xy$Y,method='nb',trControl= trainControl(
#  method = "cv",
#  number =10,
#  verboseIter = TRUE))
# predictions_naive_bayes<-predict.train(object=model_naive_bayes,test_xy[,-1])
nb_train <- naiveBayes(Y ~ ., data = train_xy)
pred_svm<-predict(nb_train, newdata = test_xy)
prop.table(table(test_xy$Y,pred_svm))
cm= as.matrix(table(test_xy$Y,pred_svm))
CV[k]<- sum(diag(cm)) / sum(cm)
#cf<-confusionMatrix(predictions_naive_bayes,test_xy$Y)
#CV[k]<- cf$overall["Accuracy"]
}
CVerror= sum(CV)/length(CV)
CV
CVerror#0.69
library(rmarkdown)
render("rapport_louis.Rmd")
options(Encoding="UTF-8")
parole <- read.csv("data/parole_train.txt", sep =" ")
n=nrow(parole)
ntrain=ceiling(n*2/3)
ntst=n-ntrain
train<-sample(1:n,ntrain)
parole.test<-parole[-train,]
parole.train<-parole[train,]
summary(character)
summary(parole)
summary(parole)
head(parole)
table(parole$y)
sum(table(parole$y))
588/2250
365/2250
sum(parole[2,])
sum(parole[2,-257])
sum(parole[1,-257])
sum(parole[5,-257])
sum(parole[5,-257])
sum(parole[,-257])
3910/(2250*256)
save.image("~/SY19/envlouis.RData")
