---
title: "SY19 TP2"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load("~/SY19/envlouis.RData")
library(caret)
library(car)
library("e1071")
library(randomForest)
library(kernlab)
library(stats)
library(MASS)
options(Encoding="UTF-8")
```

## Introduction

Dans ce TP, nous disposons de trois jeux de données. Le premier représente des expressions de visage, le second des lettres de l'alphabet et le troisième des sons. Le but est alors de trouver des classifieurs efficaces pour ces trois jeux de données en utilisant des méthodes d'apprentissage supervisé.

## Character
### Analyse

Nous commenÃ§ons dans un premier temps par analyser notre jeu de donnÃ©es. Grâce à la commande 'summary', nous pouvons voir que chaque élément est décrit par 16 variables quantitatives et que nos individus vont logiquement être divisés en 26 classes représentant l'alphabet. On note également que les variables semblent issues d'une loi centrée autour de 0. Enfin les individus sont repartis assez Ã©quitablement dans les differentes classes. 

```{r character}
table(character_data$Y)
```

### Approche 

Pour trouver le meilleur classifieur pour ces données, nous allons appliquer plusieurs méthodes étudiées en cours. Nous pourrons alors comparer l'efficacité de ces méthodes en comparant l'erreur de chaque classifieur fournie par validation croisÃ©e. Nous allons détailler certaines méthodes ci-dessous et l'ensemble des méthodes appliquées avec leurs résultats seront présentées ultérieurement dans un tableau récapitulatif.

### Application

Dans un premier temps, nous allons appliquer simplement plusieurs mÃ©thodes conjointement avec la validation croisÃ©e pour obtenir des rÃ©sultats comparables et significatifs. Nous commenÃ§ons donc par diviser nos donnÃ©es en deux parties, un ensemble train comportant les deux tiers des donnÃ©es et un ensemble de test comportant le reste.

```{r echo=FALSE}
character <- read.csv("data/characters_train.txt", sep =" ")
n=nrow(character)
ntrain=ceiling(n*2/3)
ntst=n-ntrain
train<-sample(1:n,ntrain)
character.test<-character[-train,]
character.train<-character[train,]
```
Une fois la sÃ©paration faite, nous pouvons alors commencer Ã  tester diffÃ©rents modÃ¨les pour avoir une premiÃ¨re idÃ©e des performances de chacun. Pour cela, on entraine notre modÃ¨le sur la partie 'train' et nous prÃ©disons nos classes sur la partie 'test'. Cependant, pour comparer les diffÃ©rentes mÃ©thodes il est plus judicieux d'avoir recours Ã  la validation croisÃ©e. Nous mettons alors en place ce systÃ¨me qui nous permettra d'avoir des rÃ©sultats plus significatifs pour comparer nos classifieurs. Nous divisons alors nos donnÃ©es en dix parties de mÃªme taille, Ã  chaque itÃ©ration de notre boucle nous entraÃ®nons notre modÃ¨le sur la partie des donnÃ©es de 'train' et ensuite nous pouvons prÃ©dire les donnÃ©es de test. A chaque itÃ©ration un nouvel ensemble de 'test' et donc de 'train' sont utilisÃ©s. On fait alors une moyenne des rÃ©sultats que nous avons obtenu pour obtenir une erreur stable et significative qu'on utilisera pour comparer nos differentes mÃ©thodes et ensuite choisir celle qui nous donne la plus petite erreur. 

AprÃ¨s avoir testÃ© nos differents modÃ¨les, nous avons Ã©galement mis en place une mÃ©thode de rÃ©duction de la dimension (ACP). Les donnÃ©es Ã©tant dÃ©jÃ  centrÃ©es autour de 0 et avec des valeurs assez proches, nous n'avons ni besoin de normaliser ces donnÃ©es ni de les redimensionner avant d'appliquer l'ACP.

Pour ce jeu de donnÃ©es, c'est le modÃ¨le du random foret qui a donnÃ© le meilleur rÃ©sultat. 
```{r echo=TRUE}
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n)) 
CV<-rep(0,10)
for (k in 1:n_folds) {
  test_i <- which(folds_i == k)
  train_xy <- character[-test_i, ]
  test_xy <- character[test_i, ]
  rf <- randomForest(Y ~ ., data = train_xy)
  pred_rf<-predict(rf, newdata = test_xy, type = "response")
  prop.table(table(test_xy$Y,pred_rf))
  cm= as.matrix(table(test_xy$Y,pred_rf))
  CV[k]<- sum(diag(cm)) / sum(cm)
}
CVerror= sum(CV)/length(CV)
```
En effet, pour ce jeu de donnÃ©es les mÃ©thodes de rÃ©solution linÃ©aire sont moins efficaces. C'est pour cette raison que le LDA a des performances moindres comparÃ© au random forest qui est moins dÃ©pendant des variables. De mÃªme on peut remarquer que le QDA nous donne de meilleures performances que le LDA. Ceci est probablement expliquÃ© par le fait que les variables n'ont pas les mÃªmes matrices de variance-covariance. Dans ce cas, les mÃ©thodes linÃ©aires comme le LDA sont beaucoup moins efficaces pour differencier les differentes classes. Dans ce cas de figure, les mÃ©thodes quadratiques comme le QDA ainsi que d'autres mÃ©thodes moins dÃ©pendantes de cette caractÃ©ristique comme le random forest nous donne de meilleurs rÃ©sultats.

### RÃ©sultats
Voici les rÃ©sultats de prÃ©cision obtenus pour les differentes mÃ©thodes testÃ©es:

  * RandomForest: 0.9343
  * SVM: 0.913
  * Naive-Bayes: 0.69
  * LDA: 0.70
  * QDA: 0.88
  * RDA: 0.87
  * SVM + PCA: 0.71
  * Naive-Bayes + PCA: 0.64
  * LDA + PCA: 0.65
  * QDA + PCA: 0.68
  
Comme prÃ©vu, les mÃ©thodes simples et linÃ©aires sont celles qui nous donnent les classifieurs les moins prÃ©cis. Naive-Bayes est ici peu performant ce qui pourrait s'expliquer par une trop grande corrÃ©lation entre plusieurs prÃ©dicteurs.  
Nous avons alors ensuite ajoutÃ© une partie de traitement des donnÃ©es avec l'ACP. En effet, notamment pour amÃ©liorer les rÃ©sultats de modÃ¨les comme Naive-Bayes nous avons appliquÃ© l'ACP pour rÃ©duire les dimensions de notre jeu de donnÃ©es. Cependant, quel que soit le modÃ¨le auquel on a appliquÃ© l'ACP, le rÃ©sultat de notre classifieur devenait moins bon. Ceci s'explique par le fonctionnement de ce traitement. En effet, l'ACP prend en compte seulement les coordonnÃ©es des points de nos donnÃ©es. L'ACP peut donc supprimer des informations qui sont pourtant importantes pour classifier nos donnÃ©es. En fonction de notre jeu de donnÃ©es ce processus de construction peut mener Ã  de mauvaises composantes n'expliquant pas bien nos classes et donc rÃ©sultant en des classifieurs moins performants. C'est pour cette raison que cette phase de traitement n'a pas Ã©tÃ© maintenue pour notre classifieur final.

### Conclusion

AprÃ¨s le test de nos diffÃ©rents modÃ¨les et mÃªme de l'ajout d'une phase de traitement des donnÃ©es, les rÃ©sultats obtenus par validation croisÃ©e nous permettent de choisir le meilleur classifieur pour ce jeu de donnÃ©es. Ainsi c'est le randomForest qui nous donne le meilleur rÃ©sultat avec une erreur de seulement 6.6% ce qui est assez satisfaisant. 


## Paroles

### Analyse

Comme pour le jeu de donnÃ©es prÃ©cÃ©dent, nous commenÃ§ons par analyser notre jeu de donnÃ©es avec la commande 'summary'. Nous avons donc prÃ¨s de 2500 individus caractÃ©risÃ©s par 256 variables. Ces individus sont rÃ©partis dans cinq classes.  
```{r }
table(parole$y)
```
Nous pouvons voir que les diffÃ©rentes classes ne contiennent pas le mÃªme nombre d'individus mais les diffÃ©rences sont relativement faibles (entre 16 et 26%). On note Ã©galement que les variables semblent issues d'une loi centrÃ©e autour de 0.

### Approche

Nous allons aborder ce problÃ¨me de la mÃªme maniÃ¨re que le prÃ©cÃ©dent. Nous allons comparer diffÃ©rents modÃ¨les en comparant l'erreur issue de la validation croisÃ©e et pourrons ainsi choisir notre meilleur classifieur pour ce jeu de donnÃ©es.

### Application

De la mÃªme maniÃ¨re que pour le jeu de donnÃ©es prÃ©cÃ©dent, nous allons crÃ©er notre boucle de validation croisÃ©e de dix itÃ©rations et Ã  l'intÃ©rieur de celles ci nous allons entraÃ®ner et tester nos modÃ¨les.  
De mÃªme, nous avons Ã©galement mis en place une mÃ©thode de rÃ©duction de la dimension (ACP). Les donnÃ©es Ã©tant dÃ©jÃ  centrÃ©es autour de 0 et avec des valeurs assez proches, nous n'avons ni besoin de normaliser ces donnÃ©es ni de les redimensionner avant d'appliquer l'ACP.
```{r echo=FALSE}
parole <- read.csv("data/parole_train.txt", sep =" ")
n=nrow(parole)
```

```{r echo=TRUE}
n_folds <- 10
folds_i <- sample(rep(1:n_folds, length.out = n))
CV<-rep(0,10)
for (k in 1:n_folds) {
  test_i <- which(folds_i == k)
  train_xy <- parole[-test_i, ]
  test_xy <- parole[test_i, ]
  svm_train <- svm(y ~ ., data = train_xy)
  pred_svm<-predict(svm_train, newdata = test_xy, type = "response")
  prop.table(table(test_xy$y,pred_svm))
  cm= as.matrix(table(test_xy$y,pred_svm))
  CV[k]<- sum(diag(cm)) / sum(cm)
}
CVerror= sum(CV)/length(CV)
CV
CVerror
```
Ici, c'est le SVM qui nous donne le meilleur rÃ©sultat.
### Analyse
Voici les rÃ©sultats de prÃ©cision obtenus pour les differentes mÃ©thodes testÃ©es:

  * SVM: 0.926
  * RandomForest: 0.918
  * LDA: 0.917
  * RDA: 0.903
  * Naive-Bayes: 0.88
  * QDA: 0.66
  * SVM + PCA: 0.66
  * Naive-Bayes + PCA: 0.57
  * LDA + PCA: 0.56
  * QDA + PCA: 0.63
  
On remarque ici que quatre modèles ont des performances trÃ¨s similaires Ã  savoir SVM, RandomForest, LDA ainsi que RDA. D'après les résultats, c'est le SVM qui est le meilleur classifieur pour ce jeu de données. Cependant, on voit que le LDA a un score très proche or il est souvent plus simple d'interpréter les résultats avec le LDA qu'avec le SVM. Pour ce TP, nous allons tout de même garder le meilleur score pour notre classifieur puisqu'on cherche le classifieur le plus précis possible.
On peut remarquer qu'ici aussi en ajoutant une phase de traitement des donnÃ©es avec rÃ©duction des dimensions grÃ¢ce Ã  l'ACP, les performances des classifieurs sont diminuÃ©es. Encore une fois, ceci s'explique probablement par de la perte d'informations lors de la crÃ©ation des composantes. 

### Conclusion

AprÃ¨s avoir testé différents modèles pour ce jeu de données et mis en place une phase de traitement des données, les rÃ©sultats obtenus par validation croisÃ©e nous permettent de choisir le meilleur classifieur. Ainsi c'est le SVM qui nous donne le meilleur rÃ©sultat avec une erreur de seulement 7.4% ce qui est assez satisfaisant. 